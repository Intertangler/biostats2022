{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO3eIaTWBf/+fare31UtTiM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Intertangler/biostats2022/blob/main/05_error__.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfYS905FnnQ4"
      },
      "source": [
        "# computational demo of the central limit theorem\n",
        "Let's say we are measuring the volume of liquid using a pipette. There are several independent variables that lead to error. For example: the temperature, the amount of dust in the pipette, the taper of the pipette tip, the loss of droplets on the walls of the pipette, etc etc.\n",
        "\n",
        "Below, we demonstrate the CLT by summing independent random values from -1 to 1. Observe the emergence of the Gaussian distribution by increasing the number of variables that get summed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP2u76OyoIZv"
      },
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "number_of_measuremts = 1000   # try increasing this number\n",
        "number_of_error_variables = 5 # modify this line to add more independent variables\n",
        "\n",
        "data = []\n",
        "for measurement in range(number_of_measuremts):\n",
        "  sum_variables = 0\n",
        "  for i in range(number_of_error_variables):\n",
        "    variable_value = random.uniform(-1,1)\n",
        "    sum_variables += variable_value\n",
        "  data += [sum_variables]\n",
        "\n",
        "plt.hist(data, bins=\"auto\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cV8o9Ba_so5Y"
      },
      "source": [
        "# Problem 5.0 computing the Gaussian integral (integral = area = probability)\n",
        "\n",
        "In the lectures we talked about the idea of the area under the curve and its significance to probability distributions as representing the value X weighted by probability. In the case of the Gaussian distribution, if our X value is the Z statistic i.e. our distribution is in fact the \"unit Gaussian\", then we can define an X-range in terms of standard deviations away from the mean. The area under the curve then takes on a special meaning in this case - it is the probability or confidence associated with that particular interval - e.g. 2 standard deviations corresponding to 0.9545 probability.\n",
        "\n",
        "To make this point clearer, let's generate our own Gaussian lookup-table (actually just a 1D list for us) like the ones presented in the slides. We will do this by solving the Gaussian integral numerically. To perform an integral numerically, we can use \"Euler's method\" - essentially computing the area of a tiny X range \"dx\" = x1-x2 multiplied by its corresponding height of the curve f(dx) ~ f(x1) ~ f(x2). This approximation, while not perfect, will give us a good estimate.\n",
        "\n",
        "Below we have a rough skeleton of the code you will use - fill in the missing portions:\n",
        "1. a function to compute the probability density of the Gaussian\n",
        "2. a loop that produces a list of Gaussian integrals for different confidence intervals (i.e. factors of sigma away from the mean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_jLCxntwkt-"
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def gaussian(x, mu, sigma):\n",
        "  f = ### your code here ###\n",
        "  return f\n",
        "\n",
        "# test out your gaussian function:\n",
        "fcurve = []\n",
        "for x in np.linspace(-3,3, 100):\n",
        "  f = gaussian(x, mu = 0, sigma = 1)\n",
        "  fcurve += [f]\n",
        "plt.plot(np.linspace(-3,3, 100), fcurve)\n",
        "plt.xlabel(\"Z\")\n",
        "\n",
        "\n",
        "def compute_integral(N_bins, interval):\n",
        "  mu = 0\n",
        "  sigma = 1\n",
        "  a = -1*interval\n",
        "  b = interval\n",
        "  dx = (a - b)/N_bins\n",
        "  area = 0\n",
        "  for x in np.linspace(a, b, N_bins): \n",
        "    area += ### your code here ###\n",
        "  return(area)\n",
        "\n",
        "\n",
        "for Z in np.linspace(0, 3, 100):\n",
        "  print(Z , compute_integral(N_bins = 1000, interval = Z))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbWbF83i6szA"
      },
      "source": [
        "# Problem 5.1 standard error of the mean\n",
        "Write your own function to compute the standard error of the mean from a dataset input. \n",
        "\n",
        "The code below will auto-generate a random dataset of mass values. Anayze the datasets produced - what tends to happen as you increase the number of datapoints?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ISCtHM9w6yfP"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "## paste your mean, std, var, etc functions here\n",
        "\n",
        "def sem(my_data, sigma):\n",
        "  my_sem = ## your code here ##\n",
        "  return my_sem\n",
        "\n",
        "\n",
        "true_mean = 12\n",
        "mass_data = []\n",
        "sample_sizes = [10,50,100, 500, 1000, 5000]\n",
        "sem_data = []\n",
        "sigma = 1.5\n",
        "for sample_size_N in sample_sizes:\n",
        "  for measurement in range(sample_size_N):\n",
        "    mass_meas = np.random.normal(loc=true_mean, scale=1.2, size=None)\n",
        "    mass_data += [mass_meas]\n",
        "  sem_data += [sem(mass_data, sigma)]\n",
        "\n",
        "\n",
        "print(sem(mass_data, sigma))\n",
        "\n",
        "# optional analysis and plot of SEM with different sample sizes N\n",
        "plt.plot(sample_sizes, sem_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ1STFG6iVb-"
      },
      "source": [
        "# measurement error\n",
        "In experimental sciences, it is very common to estimate the concentrations of dissolved chemicals using absorbance spectroscopy or spectrophotometry. I.e. the darker the solution, the more solute is present in it. This technique can be used to measure how much DNA is dissolved in water, since DNA absorbs UV light at a wavelength of 260 nm.\n",
        "\n",
        "In a typical absorbance spectroscopy experiment, concentration is calculated using the Beer-Lambert equation:\n",
        "\n",
        "$c = \\frac{A}{\\epsilon l}$ where A is the absorbance (the fraction of light that is able to pass through the sample relative to the source light), $\\epsilon$ is a molar extinction coefficient equal to (0.027 (μg/ml)^−1^cm−1 in the case of single stranded DNA, and $l$ is the pathlength or distance that the light has to travel through the sample. \n",
        "\n",
        "The code below will generate a dataset of absorbance measurements and pathlength measurements.\n",
        "\n",
        "Just run it, and move on to the next part below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmzIcm5y74QD"
      },
      "source": [
        "### run this block of code ###\n",
        "import os.path\n",
        "import sys\n",
        "from os import path\n",
        "import numpy as np\n",
        "\n",
        "if path.exists(\"/content/biostats2021/\"):\n",
        "    pass\n",
        "else:\n",
        "    !git clone https://github.com/Intertangler/biostats2021.git\n",
        "sys.path.insert(0,'/content')\n",
        "data = np.loadtxt('/content/biostats2021/absorbance.txt')\n",
        "\n",
        "experiments = 100 \n",
        "extinction_coeff = 0.027 # (μg/ml)^−1^cm−1\n",
        "\n",
        "### data generator - do not modify ###\n",
        "abs_data = [] # absorbance\n",
        "pathl_data = [] # cm\n",
        "for experiment in range(experiments):\n",
        "  abs_data += [max(0,np.random.normal(loc=data[0], scale=0.01198273, size=None))]\n",
        "  pathl_data += [max(0,np.random.normal(loc=data[1], scale=0.1354, size=None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_vbTRNZ-PhZ"
      },
      "source": [
        "# Problem 5.2 measurement\n",
        "Examine the abs_data object that has been generated above - this is our absorbance data. \n",
        "\n",
        "Compute the sample mean and standard deviation. Do the same for the pathlength data pathl_data.\n",
        "\n",
        "Compute the mean concentration using the Beer Lambert formula and the mean values of absorbance and pathlength, as well as the extinction coefficient for DNA. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSrLx3CNoyEQ"
      },
      "source": [
        "mean_abs = ## your code \n",
        "mean_pathl = ## your code \n",
        "std_abs = ## your code \n",
        "std_pathl = ## your code \n",
        "mean_conc = ## your code \n",
        "print(mean_conc, \"μg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEqwcpPbCJsN"
      },
      "source": [
        "Now we want to know the uncertainty of the force that we got from our calculation. To do this, we employ the law of combination of errors, also known as the error propagation method.\n",
        "\n",
        "Error propagation formula:\n",
        "\n",
        "$\\sigma_C = ((\\frac{dC}{dl})^2\\sigma_l^2 + (\\frac{dC}{dA})^2\\sigma_A^2)^{1/2}$ \n",
        "\n",
        "$C = \\frac{A}{\\epsilon l}$\n",
        "\n",
        "$\\frac{dC}{dl} = \\frac{d}{dl}C = \\frac{d}{dl}(\\frac{A}{\\epsilon l}) = -\\frac{A}{\\epsilon l^2}$\n",
        "\n",
        "$\\frac{dC}{dA} = \\frac{d}{dA}C = \\frac{d}{dA}(\\frac{A}{\\epsilon l}) = \\frac{1}{\\epsilon l}$\n",
        "\n",
        "$\\sigma_C = ((?)^2\\sigma_l^2 + (?)^2\\sigma_A^2)^{1/2}$ \n",
        "\n",
        "Compute the standard deviation now for the concentration with the formula above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMiHAT5Befq1"
      },
      "source": [
        "\n",
        "\n",
        "std_conc = ## your code \n",
        "print(\"concentration = \", mean_conc, \" ± \", std_conc, \" μg\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}